{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model utilizes a DCGAN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\site-packages\\torch\\__init__.py:764\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m    763\u001b[0m __name, __obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    766\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(__name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from pytorch_model_summary import summary\n",
    "import torch.optim as optim\n",
    "import music21\n",
    "import matplotlib as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "NOISE_DIM = 100\n",
    "NUM_CLASSES = 18\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label embedding layer:  torch.Size([1, 8, 16, 1])\n",
      "Noise embedding layer:  torch.Size([1, 128, 8, 16])\n",
      "After concatenation:  torch.Size([1, 129, 8, 16])\n",
      "First layer:  torch.Size([1, 128, 16, 32])\n",
      "Second layer:  torch.Size([1, 64, 32, 64])\n",
      "Third layer:  torch.Size([1, 32, 64, 128])\n",
      "Final layer:  torch.Size([1, 1, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Embedding for the labels (num_classes = 18 for each composer)\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)  # (N, 50)\n",
    "\n",
    "        # Dense layer for the label embedding\n",
    "        self.fc_label = nn.Linear(50, 8 * 16)  # (N, 128) for reshaping to 8x16\n",
    "\n",
    "        # Dense layer for the latent noise input\n",
    "        self.fc_noise = nn.Linear(latent_dim, 128 * 8 * 16)  # (N, 128 * 8 * 16)\n",
    "\n",
    "        # ConvTranspose layers for upsampling\n",
    "        self.conv1 = nn.ConvTranspose2d(129, 128, kernel_size=4, stride=2, padding=1)  # Upsample to 16x32\n",
    "        self.conv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)   # Upsample to 32x64\n",
    "        self.conv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)    # Upsample to 64x128\n",
    "        self.conv4 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)     # Upsample to 128x256\n",
    "\n",
    "        # LeakyReLU activation function\n",
    "        self.activation = nn.ReLU(0.2)\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        # Step 1: Embed the labels\n",
    "        label_embedding = self.label_emb(labels)  # (N, 50)\n",
    "        label_embedding = self.fc_label(label_embedding)  # (N, 128)\n",
    "        label_embedding = label_embedding.view(-1, 8, 16, 1)  # Reshape to (N, 8, 16, 1)\n",
    "        print(\"Label embedding layer: \", label_embedding.shape)\n",
    "        # Step 2: Process the latent noise input through a dense layer\n",
    "        noise_embedding = self.fc_noise(noise)  # (N, 128 * 8 * 16)\n",
    "        noise_embedding = self.activation(noise_embedding)  # Apply LeakyReLU\n",
    "        noise_embedding = noise_embedding.view(-1, 128, 8, 16)  # Reshape to (N, 128, 8, 16)\n",
    "        print(\"Noise embedding layer: \", noise_embedding.shape)\n",
    "        # Step 3: Concatenate the noise and label embeddings along the channel axis\n",
    "        x = torch.cat((noise_embedding, label_embedding.permute(0, 3, 1, 2)), dim=1)  # (N, 129, 8, 16)\n",
    "        print(\"After concatenation: \", x.shape)\n",
    "        # Step 4: Upsample to 16x32\n",
    "        x = self.conv1(x)  # (N, 128, 16, 32)\n",
    "        x = self.activation(x)\n",
    "        print(\"First layer: \", x.shape)\n",
    "        # Step 5: Upsample to 32x64\n",
    "        x = self.conv2(x)  # (N, 64, 32, 64)\n",
    "        x = self.activation(x)\n",
    "        print(\"Second layer: \", x.shape)\n",
    "        # Step 6: Upsample to 64x128\n",
    "        x = self.conv3(x)  # (N, 32, 64, 128)\n",
    "        x = self.activation(x)\n",
    "        print(\"Third layer: \", x.shape)\n",
    "        # Step 7: Upsample to 128x256\n",
    "        out = self.conv4(x)  # (N, 1, 128, 256)\n",
    "        print(\"Final layer: \", out.shape)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "         # Step 9: Apply thresholding to get binary output (0 or 1)\n",
    "        out_binary = (out > 0.5).float()  # Convert to 0 or 1\n",
    "\n",
    "        return out_binary\n",
    "\n",
    "# Instantiate the model\n",
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "model = Generator(latent_dim, num_classes)\n",
    "\n",
    "# Example input (noise and labels)\n",
    "batch_size = 1\n",
    "noise = torch.randn(batch_size, latent_dim)  # (N, latent_dim)\n",
    "labels = torch.randint(0, num_classes, (batch_size,))  # (N,)\n",
    "\n",
    "# Forward pass\n",
    "output = model(noise, labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label embedding:  torch.Size([1, 1, 128, 256])\n",
      "Concatenation:  torch.Size([1, 2, 128, 256])\n",
      "First layer:  torch.Size([1, 128, 64, 128])\n",
      "Second layer:  torch.Size([1, 128, 32, 64])\n",
      "Flatten  torch.Size([1, 262144])\n",
      "Final:  torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "tensor([[0.5015]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_shape=(1, 128, 256), num_classes=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Embedding for labels\n",
    "        self.label_emb = nn.Embedding(num_classes, 50)  # (N, 50)\n",
    "        \n",
    "        # Fully connected layer for label embedding, to match pianoroll matrix size\n",
    "        self.fc_label = nn.Linear(50, in_shape[1] * in_shape[2])  # (N, 128 * 256)\n",
    "        \n",
    "        # Convolutional layers for processing the concatenated image and label\n",
    "        self.conv1 = nn.Conv2d(2, 128, kernel_size=3, stride=2, padding=1)  # (N, 128, 64, 128)\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)  # (N, 128, 32, 64)\n",
    "        \n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(128 * 32 * 64, 1)\n",
    "        \n",
    "        # Sigmoid activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, img, labels):\n",
    "        # Step 1: Embed the labels\n",
    "        label_embedding = self.label_emb(labels)  # (N, 50)\n",
    "        \n",
    "        # Step 2: Process label embedding through a dense layer to match image dimensions\n",
    "        label_embedding = self.fc_label(label_embedding)  # (N, 128 * 256)\n",
    "        \n",
    "        # Step 3: Reshape label embedding to add as a channel to the image\n",
    "        label_embedding = label_embedding.view(-1, 1, 128, 256)  # Reshape to (N, 1, 128, 256)\n",
    "        print(\"Label embedding: \", label_embedding.shape)\n",
    "        # Step 4: Concatenate the label embedding with the image input\n",
    "        x = torch.cat((img, label_embedding), dim=1)  # Concatenate along channel dimension: (N, 2, 128, 256)\n",
    "        print(\"Concatenation: \", x.shape)\n",
    "        # Step 5: Apply convolutional layers to process the image and label\n",
    "        x = nn.LeakyReLU(0.2)(self.conv1(x))  # (N, 128, 64, 128)\n",
    "        print(\"First layer: \", x.shape)\n",
    "        x = nn.LeakyReLU(0.2)(self.conv2(x))  # (N, 128, 32, 64)\n",
    "        print(\"Second layer: \", x.shape)\n",
    "        # Step 6: Flatten the feature maps\n",
    "        x = x.view(x.size(0), -1)  # (N, 128 * 32 * 64 = 262144)\n",
    "        print(\"Flatten \", x.shape)\n",
    "        # Step 7: Apply dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 8: Fully connected layer and sigmoid activation for binary classification\n",
    "        out = self.fc(x)  # (N, 1)\n",
    "        out = self.sigmoid(out)  # Sigmoid activation to get values between 0 and 1\n",
    "        print(\"Final: \", out.shape)\n",
    "        return out\n",
    "\n",
    "# Instantiate the discriminator\n",
    "discriminator = Discriminator(in_shape=(1, 128, 256), num_classes=10)\n",
    "\n",
    "\n",
    "# Example inputs: random image and label\n",
    "batch_size = 1\n",
    "image = torch.randn(batch_size, 1, 128, 256)  # Random image input\n",
    "#labels = torch.randint(0, 10, (batch_size,))  # Random labels (10 classes)\n",
    "\n",
    "# Forward pass\n",
    "output1 = discriminator(output, labels)\n",
    "\n",
    "print(output1.shape)  # Output should be (batch_size, 1)\n",
    "print(output1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\site-packages\\torch\\__init__.py:2143\u001b[0m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m library\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m-> 2143\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations\n\u001b[0;32m   2145\u001b[0m \u001b[38;5;66;03m# Enable CUDA Sanitizer\u001b[39;00m\n\u001b[0;32m   2146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_CUDA_SANITIZER\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\site-packages\\torch\\_meta_registrations.py:41\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pytree \u001b[38;5;28;01mas\u001b[39;00m pytree\n\u001b[0;32m     39\u001b[0m aten \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\n\u001b[1;32m---> 41\u001b[0m _meta_lib_dont_use_me_use_register_meta \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maten\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIMPL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mregister_meta\u001b[39m(op):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(fn):\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\site-packages\\torch\\library.py:69\u001b[0m, in \u001b[0;36mLibrary.__init__\u001b[1;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;129;01min\u001b[39;00m _reserved_namespaces \u001b[38;5;129;01mand\u001b[39;00m (kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEF\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFRAGMENT\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(ns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is a reserved namespace. Please try creating a library with another name.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m frame \u001b[38;5;241m=\u001b[39m \u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     70\u001b[0m filename, lineno \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mfilename, frame\u001b[38;5;241m.\u001b[39mlineno\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm: Optional[Any] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_dispatch_library(kind, ns, dispatch_key, filename, lineno)\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 227\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\traceback.py:383\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m--> 383\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\traceback.py:306\u001b[0m, in \u001b[0;36mFrameSummary.line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_line \u001b[38;5;241m=\u001b[39m \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlineno\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\linecache.py:30\u001b[0m, in \u001b[0;36mgetline\u001b[1;34m(filename, lineno, module_globals)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetline\u001b[39m(filename, lineno, module_globals\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a line for a Python source file from the cache.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Update the cache if it doesn't contain an entry for this file already.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     lines \u001b[38;5;241m=\u001b[39m \u001b[43mgetlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_globals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(lines):\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m lines[lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\linecache.py:46\u001b[0m, in \u001b[0;36mgetlines\u001b[1;34m(filename, module_globals)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cache[filename][\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdatecache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_globals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mMemoryError\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     clearcache()\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\linecache.py:137\u001b[0m, in \u001b[0;36mupdatecache\u001b[1;34m(filename, module_globals)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tokenize\u001b[38;5;241m.\u001b[39mopen(fullname) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m--> 137\u001b[0m         lines \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mSyntaxError\u001b[39;00m):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_buffer_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;66;03m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;66;03m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer_decode(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define hyperparameters\n",
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0002\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate models\n",
    "generator = Generator(latent_dim, num_classes).to(device)\n",
    "discriminator = Discriminator(in_shape=(1, 128, 256), num_classes=num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Placeholder: Create sample data (replace this with your actual dataset)\n",
    "# Random input data for music sequences (x, 128, 128) and corresponding labels (x, 1)\n",
    "num_samples = 1000\n",
    "real_data = torch.randn(num_samples, 1, 128, 256)  # (N, 1, 128, 256)\n",
    "labels = torch.randint(0, num_classes, (num_samples,))  # (N,)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(real_data, labels)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Begin training\")\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, (real_imgs, class_labels) in enumerate(dataloader):\n",
    "        batch_size = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        class_labels = class_labels.to(device)\n",
    "\n",
    "        # Generate labels for real and fake data\n",
    "        valid = torch.ones(batch_size, 1).to(device)  # Real labels\n",
    "        fake = torch.zeros(batch_size, 1).to(device)  # Fake labels\n",
    "\n",
    "        # ===================\n",
    "        # Train Discriminator\n",
    "        # ===================\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Train on real data\n",
    "        real_loss = criterion(discriminator(real_imgs, class_labels), valid)\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = torch.randn(batch_size, latent_dim).to(device)\n",
    "        gen_labels = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "        fake_imgs = generator(noise, gen_labels)\n",
    "\n",
    "        # Train on fake data\n",
    "        fake_loss = criterion(discriminator(fake_imgs.detach(), gen_labels), fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # ===================\n",
    "        # Train Generator\n",
    "        # ===================\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Generate fake images and calculate loss\n",
    "        gen_imgs = generator(noise, gen_labels)\n",
    "        g_loss = criterion(discriminator(gen_imgs, gen_labels), valid)\n",
    "\n",
    "        # Backprop and optimize\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Print training progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], \"\n",
    "                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Music_gen_WS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
