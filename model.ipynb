{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model utilizes a DCGAN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "File \u001b[1;32mc:\\Users\\Sam\\anaconda3\\envs\\Music_gen_WS\\lib\\site-packages\\torch\\__init__.py:764\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[0;32m    763\u001b[0m __name, __obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    766\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(__name)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import music21\n",
    "import matplotlib as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "torch.cuda.empty_cache()\n",
    "cudnn.benchmark = True  # Optimise for hardware\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "NOISE_DIM= 100\n",
    "NUM_CLASSES = 18\n",
    "BETA1 = 0.5 # Hyperparamter for adam optimizer\n",
    "LRD = 0.0001 # Might need to adjust\n",
    "LRG = 0.002\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label embedding layer:  torch.Size([1, 8, 16, 1])\n",
      "Noise embedding layer:  torch.Size([1, 128, 8, 16])\n",
      "After concatenation:  torch.Size([1, 129, 8, 16])\n",
      "First layer:  torch.Size([1, 128, 16, 32])\n",
      "Second layer:  torch.Size([1, 64, 32, 64])\n",
      "Third layer:  torch.Size([1, 32, 64, 128])\n",
      "Final layer:  torch.Size([1, 1, 128, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "NUM_CLASSES = 18  # Set this to your actual number of classes\n",
    "EMBEDDING_DIM = 50  # Example embedding dimension; adjust as needed\n",
    "NOISE_DIM = 100  # Example noise dimension; adjust as needed\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Embedding for the labels\n",
    "        self.label_emb = nn.Embedding(NUM_CLASSES, EMBEDDING_DIM)  # (N, 50)\n",
    "\n",
    "        # Dense layer for the label embedding\n",
    "        self.fc_label = nn.Linear(EMBEDDING_DIM, 8 * 16)  # (N, 128)\n",
    "\n",
    "        # Dense layer for the latent noise input\n",
    "        self.fc_noise = nn.Linear(NOISE_DIM, 128 * 8 * 16)  # (N, 128 * 8 * 16)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(129, 128, kernel_size=4, stride=2, padding=1),  # Upsample to 16x32\n",
    "            nn.BatchNorm2d(128), nn.ReLU(0.2),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # Upsample to 32x64\n",
    "            nn.BatchNorm2d(64), nn.ReLU(0.2),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # Upsample to 64x128\n",
    "            nn.BatchNorm2d(32), nn.ReLU(0.2),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)     # Upsample to 128x256\n",
    "        )\n",
    "\n",
    "        #self._initialize_weights()\n",
    "        \n",
    "    def forward(self, noise, labels):\n",
    "       \n",
    "        # Step 1: Embed the labels\n",
    "        label_embedding = self.label_emb(labels)  # (N, EMBEDDING_DIM)\n",
    "        \n",
    "        label_embedding = self.fc_label(label_embedding)  # (N, 128)\n",
    "    \n",
    "        label_embedding = label_embedding.view(BATCH_SIZE, 8, 16, 1)  # Reshape to (N, 8, 16, 1)\n",
    "        \n",
    "        # Step 2: Process the latent noise input through a dense layer\n",
    "        noise_embedding = self.fc_noise(noise)  # (N, 128 * 8 * 16)\n",
    "        #noise_embedding = torch.relu(noise_embedding)  # Apply ReLU\n",
    "        noise_embedding = noise_embedding.view(-1, 128, 8, 16)  # Reshape to (N, 128, 8, 16)\n",
    "\n",
    "        # Step 3: Concatenate the noise and label embeddings along the channel axis\n",
    "        x = torch.cat((noise_embedding, label_embedding.permute(0, 3, 1, 2)), dim=1)  # (N, 129, 8, 16)\n",
    "\n",
    "        x = self.model(x)  # Apply the transposed convolutional layers\n",
    "\n",
    "        # Step 9: Apply sigmoid activation to get output in range [0, 1]\n",
    "        out = torch.sigmoid(x)  # Sigmoid activation for binary output\n",
    "\n",
    "        return out  # Return the outp\n",
    "    \n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.data *= 0.1\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "                m.weight.data *= 0.1\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.data *= 0.1\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "latent_dim = 100\n",
    "num_classes = 18\n",
    "model = Generator()\n",
    "\n",
    "# Example input (noise and labels)\n",
    "batch_size = 64\n",
    "noise = torch.randn(batch_size, latent_dim)  # (N, latent_dim)\n",
    "labels = torch.randint(0, num_classes, (batch_size,))  # (N,)\n",
    "\n",
    "# Forward pass\n",
    "output = model(noise, labels)\n",
    "print(output.shape)  # Should be (batch_size, 1, 128, 256)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label embedding:  torch.Size([1, 1, 128, 256])\n",
      "Concatenation:  torch.Size([1, 2, 128, 256])\n",
      "First layer:  torch.Size([1, 128, 64, 128])\n",
      "Second layer:  torch.Size([1, 128, 32, 64])\n",
      "Flatten  torch.Size([1, 262144])\n",
      "Final:  torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "tensor([[0.5015]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_shape=(1, 128, 256), num_classes=NUM_CLASSES):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(num_classes, EMBEDDING_DIM)\n",
    "\n",
    "        # The output dimension after embedding will be the same as the input shape\n",
    "        self.fc_label = nn.Linear(EMBEDDING_DIM, in_shape[1] * in_shape[2])\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(2, 128, kernel_size=3, stride=2, padding=1),  # (2, 128, 256) -> (128, 64, 128)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),  # (128, 64, 128) -> (128, 32, 64)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=2, padding=1),   # (128, 32, 64) -> (64, 16, 32)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),    # (64, 16, 32) -> (64, 8, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2, padding=1),    # (64, 8, 16) -> (32, 4, 8)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=2, padding=1)      # (32, 4, 8) -> (1, 2, 4)\n",
    "        )\n",
    "        \n",
    "        #self._initialize_weights()\n",
    "    def forward(self, pr, labels):\n",
    "        # Step 1: Embed the labels\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        label_embedding = self.fc_label(label_embedding)\n",
    "        label_embedding = label_embedding.view(BATCH_SIZE, 1, 128, 256)  # Reshape to (N, 1, 128, 256)\n",
    "        \n",
    "        # Step 2: Concatenate image and label embeddings along the channel dimension\n",
    "        x = torch.cat((pr, label_embedding), dim=1)  # Concatenate along channel dimension\n",
    "\n",
    "        # Step 3: Pass through the model\n",
    "        x = self.model(x)  # Apply the convolutional layers\n",
    "        \n",
    "        # Step 4: Average over height and width to get a single output\n",
    "        out = torch.mean(x, dim=[2, 3])  # Global average pooling\n",
    "        out = torch.sigmoid(out)  # Apply sigmoid activation for output\n",
    "\n",
    "        return out  # Final output will be in range [0, 1]\n",
    "\n",
    "\n",
    "    def _initialize_weights(self) -> None:\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.data *= 0.1\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "                m.weight.data *= 0.1\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.data *= 0.1\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "# Example of usage\n",
    "discriminator = Discriminator(in_shape=(1, 128, 256), num_classes=10)\n",
    "\n",
    "# Example inputs: random image and label\n",
    "batch_size = 64\n",
    "output = torch.randn(batch_size, 1, 128, 256)  # Random image input\n",
    "labels = torch.randint(0, 10, (batch_size,))  # Random labels (10 classes)\n",
    "\n",
    "# Forward pass\n",
    "output1 = discriminator(output, labels)\n",
    "\n",
    "print(output1.shape)  # Output should be (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "inputs_seq = torch.load(\"Input_tensors.pt\")\n",
    "labels_seq = torch.load(\"Labels_tensors.pt\")\n",
    "dataset = TensorDataset(inputs_seq, labels_seq)\n",
    "\n",
    "#Split into batches\n",
    "batch_size = BATCH_SIZE\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Check that the data is loaded correctly\n",
    "print(\"Number of input pianorolls: \", len(dataset))\n",
    "print(\"Length of dataloader: \", len(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``netG`` and ``netD``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the models to the GPU\n",
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# Initialize the labels\n",
    "real_label = 0.95\n",
    "fake_label = 0\n",
    "\n",
    "fixed_noise = torch.randn(BATCH_SIZE, NOISE_DIM,  device=device)\n",
    "\n",
    "# Define the optimizers\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=LRG, betas=(BETA1, 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=LRD, betas=(BETA1, 0.999))\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(\"Generator model: \", netG)\n",
    "print(\"Discriminator model: \", netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lists to keep track of progress\n",
    "output_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "\"\"\" \n",
    "Data is being loaded correctly. For some reason, model keeps blowing up and not fucking working. D(x)should start close to 1 (predicting everything as real) before settling around 0.5\n",
    "D(g(z)) should start around 0 (everything should be picked as fake) before sitting at 0.5.\n",
    "\n",
    "This means the discriminator can't determine which is real and which is fake, so generator has been trained sufficiently.\n",
    "\n",
    "I think this could be errors in the data or data normalization process\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(30):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader):\n",
    "        \n",
    "        ##########\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        \n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device).float()   # ensure tensor is float32\n",
    "        real_cpu = real_cpu.unsqueeze(1)\n",
    "        \n",
    "        b_size = real_cpu.size(0)\n",
    "        \n",
    "        if b_size != BATCH_SIZE:\n",
    "            #print(f\"Skipping batch {i} due to insufficient size: {b_size}\")\n",
    "            continue\n",
    "        \n",
    "        real_label_from_data = data[1].to(device).int()     # Generator and discriminator expect integer labels\n",
    "        \n",
    "    \n",
    "        # Creates a label tensor of 1s to pass to loss function\n",
    "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
    "        netD.zero_grad()\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu, real_label_from_data).view(-1)\n",
    "        \n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, NOISE_DIM, dtype=torch.float, device=device)\n",
    "\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise, real_label_from_data)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach(), real_label_from_data).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "    \n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake, real_label_from_data).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 5 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 10 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, real_label_from_data).detach().cpu()   # Move back to CPU so it can be used.\n",
    "            \n",
    "            output_list.append(fake)\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21 as m21\n",
    "import os\n",
    "OUTPUT_FILES = \"training_files\"\n",
    "TIME_STEP = 0.25\n",
    "def convert_stream(matrix, format=\"midi\", file_name='output.mid',filepath=OUTPUT_FILES, step_duration=TIME_STEP):\n",
    "    \"\"\"\n",
    "    Converts the piano roll matrix back into a music 21 stream. Writes this stream to a midi file.\n",
    "    :params matrix: 2D piano roll matrix\n",
    "    :params format: format file type to write the stream\n",
    "    :params file_name: the file name of the output file\n",
    "    :params filepath: the output path of the directory holding the output files\n",
    "    :params step_duration: the size of the step on the x axis of the piano roll matrix\n",
    "    :returns None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the shape of the input matrix\n",
    "    \n",
    "    rows, cols = matrix.shape\n",
    "    matrix = (matrix > 0.51).float()\n",
    "    nulls = np.zeros((rows, 1))\n",
    "    matrix = np.hstack((matrix, nulls))\n",
    "    # Create two dictionaries. The first holds the notes that are on. The second holds each 'finished' note and its offset\n",
    "    active_notes = {}\n",
    "    note_list = {}\n",
    "\n",
    "    # Iterates through every member in the matrix\n",
    "    for col in range(cols - 1):\n",
    "        for row in range(rows - 1):\n",
    "            # Finds the midi pitch and creates a new note to represent the pitch and duration\n",
    "            midi_pitch = row\n",
    "            note = m21.note.Note(midi_pitch)\n",
    "            note.quarterLength = step_duration\n",
    "\n",
    "            # If this note is 'on':\n",
    "            if matrix[row, col] == 1:\n",
    "                # remove midi pitches outside the range of a piano\n",
    "                if midi_pitch < 21 or midi_pitch > 95:\n",
    "                    continue\n",
    "                # Checks if the note has already been turned on, or is active.\n",
    "                if midi_pitch in active_notes:\n",
    "                    \n",
    "                    # If already active, updates the step duration of the note in the dictionary\n",
    "                    lst = active_notes[midi_pitch]\n",
    "                    lst[0] = lst[0] + step_duration\n",
    "                    active_notes[midi_pitch] = lst\n",
    "\n",
    "                # If newly activated, then adds the note duration and offset items to the midi pitch key in the dictionary\n",
    "                else:\n",
    "                    note.offset = col * step_duration\n",
    "                    active_notes[midi_pitch] = [note.quarterLength, note.offset]\n",
    "\n",
    "            # If the member is off but still in acitve notes, creates a new note and removes it from the dictionary\n",
    "            elif midi_pitch in active_notes:\n",
    "                # Grabs the duration and offset of the note and creates a new note object with duraiton, offset, midi pitch attributes\n",
    "                lst = active_notes[midi_pitch]\n",
    "                note = m21.note.Note( midi_pitch)\n",
    "                note.quarterLength = lst[0]\n",
    "                note.offset = lst[1]\n",
    "                # Adds this note to the note dictionary based off of the offset\n",
    "                note_list[note.offset] = note\n",
    "                del active_notes[midi_pitch]\n",
    "                    \n",
    "    # Creates a new stream and grabs the keys (offsets) and values (note onjects) from the note list dictionary\n",
    "    new_stream = m21.stream.Stream()\n",
    "    keys = list(note_list.keys())\n",
    "    notes = list(note_list.values())\n",
    "\n",
    "    # Iterates through every item in the dictionary\n",
    "    for i in range(len(note_list)):\n",
    "        # Inserts the note based off of its offset\n",
    "        new_stream.insert(keys[i], notes[i])\n",
    "\n",
    "    # Creates the filepath for the output file\n",
    "    path = os.path.join(filepath, file_name)\n",
    "\n",
    "    # Makes the directory if it doesn't exist\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "    # Writes the stream as a midi file to the path\n",
    "    new_stream.write(format, fp=path)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(len(output_list))\n",
    "\n",
    "for i in range(len(output_list) - 1, 40, -1):\n",
    "    filename = \"Class\" + str(i) + \".mid\"\n",
    "    \n",
    "    # Visualize the first generated sample\n",
    "    \n",
    "    sample = output_list[i][0][0].cpu()  # Remove batch dimension\n",
    "    convert_stream(sample, file_name=filename)\n",
    "    plt.imshow(sample.detach().numpy(), cmap='gray')  # Use cmap='gray' for single-channel images\n",
    "    plt.axis('off')  # Turn off axis\n",
    "    plt.show()\n",
    "# Output shape should be (batch_size, 1, 128, 256)\n",
    "\n",
    "\n",
    "# If you want to visualize or save the generated samples, you can process them further\n",
    "# For example, if you are using matplotlib to visualize:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Music_gen_WS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
