{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing midi data using piano roll time series format.\n",
    "\n",
    "X axis represents time in steps of 0.25 beats.\n",
    "Y axis represents the midi value of the note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import music21 as m21\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "torch.cuda.empty_cache()\n",
    "cudnn.benchmark = True  # Optimise for hardware\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\"\"\"\n",
    "Preprocessing file used to transform piano midi files into one hot encoded tensors. Uses music21 library to translate the \n",
    "midi files into music21 objects where the note pitches and durations can be extracted. \n",
    "PyTorch is used to one hot encode and transform into output tensors. \n",
    "\"\"\"\n",
    "# Constants for datapaths of each file and other key values.\n",
    "PIANO_DATAPATH = \"Dataset/bach\"\n",
    "TIME_STEP = 0.25\n",
    "SAVE_MUSIC_PATH = \"Processed_data\"\n",
    "SEQUENCE_LENGTH = 128\n",
    "SINGLE_FILE_PATH = \"single_file_dataset.txt\"\n",
    "SONG_MAPPING_PATH = \"Song_Mapping.json\"\n",
    "LABEL_MAPPING_PATH = \"Label_Mapping.json\"\n",
    "OUTPUT_FILES = \"Output_files\"\n",
    "SIZE = 512\n",
    "\n",
    "acceptable_durations = [\n",
    "    0.25, \n",
    "    0.5,\n",
    "    0.75,\n",
    "    1, \n",
    "    1.25,\n",
    "    1.5, \n",
    "    1.75, \n",
    "    2, \n",
    "    2.25,\n",
    "    2.5,\n",
    "    2.75,\n",
    "    3,\n",
    "    3.25,\n",
    "    3.5, \n",
    "    3.75,\n",
    "    4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"\n",
    "    This function takes in a datapath and uses the os library to iterate through every file in the datapath directories\n",
    "    and transforms them into music21 objects.\n",
    "\n",
    "    :params data path containing the directory path to a folder of midi files\n",
    "    :returns a list of music21 objects \n",
    "    \"\"\"\n",
    "    # Generate the labels for each composer.\n",
    "    songs = []\n",
    "    labels = []\n",
    "    no_songs_in_dir = []\n",
    "    \n",
    "    # Iterates through each file in every directory from the input file path\n",
    "    for path, subdir, filenames in os.walk(data_path):\n",
    "        base = os.path.basename(path)\n",
    "        labels.append(base)\n",
    "        count = 0\n",
    "        for file in filenames:\n",
    "            \n",
    "            # For every midi file, convert to a music21 object\n",
    "            if file.endswith(\"mid\"):\n",
    "                song = m21.converter.parse(os.path.join(path, file))\n",
    "                # Take just the melody of the song\n",
    "                #song = song.getElementsByClass(m21.stream.Part)\n",
    "                songs.append(song)\n",
    "                count += 1\n",
    "        no_songs_in_dir.append(count)\n",
    "\n",
    "        # Get rid of the 'Dataset' label as this doesn't represent anything meaningful\n",
    "    if labels[0] == 'Dataset':\n",
    "        labels = labels[1:]\n",
    "        no_songs_in_dir = no_songs_in_dir[1:]\n",
    "\n",
    "    return songs, labels, no_songs_in_dir\n",
    "\n",
    "songs, labels, no_songs_in_dir = load_data(PIANO_DATAPATH)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano_roll(cols, song):\n",
    "    \"\"\" \n",
    "    This function takes in a single song and column dimension and outputs the pianoroll 2D matrix that represents the midi\n",
    "    notes at every time step.\n",
    "    :params cols, song: column size of the matrix and the input song to convert\n",
    "    :returns a 2D pianoroll matrix.\n",
    "    \"\"\"\n",
    "    array = np.zeros((128, cols))\n",
    "    index = 0\n",
    "    # Iterates through every note in the song. \n",
    "    for symbol in song.flatten().notesAndRests:\n",
    "\n",
    "        if isinstance(symbol, m21.note.Note):\n",
    "            # Finds the midi symbol of the note\n",
    "            note = int(symbol.pitch.midi)\n",
    "            \n",
    "            # Finds the duration of the note\n",
    "            duration = symbol.duration.quarterLengthNoTuplets\n",
    "\n",
    "            # Converts this to the step size. If duration = 0.5 this equates to two time steps. \n",
    "            steps = int(duration // TIME_STEP)\n",
    "            \n",
    "            # Set the value of this midi note to 1 for the amount of time steps.\n",
    "            array[note, index: index + steps + 1] = 1\n",
    "            \n",
    "            # Resets the index to the next time position.\n",
    "            index += steps \n",
    "         \n",
    "        elif index > cols:\n",
    "            break\n",
    "    \n",
    "    return array\n",
    "        \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptable_note_durations(song):\n",
    "    \"\"\" \n",
    "    Removes the notes that have unaccetpable durations. \n",
    "    :params song: the song to remove the ntoes from\n",
    "    :returns the same song object with the ntoes removed.\n",
    "    \"\"\"\n",
    "    # Creates a new song object\n",
    "    new_song = m21.stream.base.Score()\n",
    "    \n",
    "    lst = []\n",
    "\n",
    "    # Iterates through every symbol in the song\n",
    "    for symbol in song.flatten().notesAndRests:\n",
    "        \n",
    "        # If the symbol has a duration that isn't acceptable, remove it from the song.\n",
    "        if symbol.duration.quarterLength not in acceptable_durations:\n",
    "            lst.append(symbol.duration.quarterLength)\n",
    "            song.remove(symbol)\n",
    "            continue\n",
    "    \n",
    "    return song\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_stream(matrix, format=\"midi\", file_name='output.mid',filepath=OUTPUT_FILES, step_duration=TIME_STEP):\n",
    "    \"\"\"\n",
    "    Converts the piano roll matrix back into a music 21 stream. Writes this stream to a midi file.\n",
    "    :params matrix: 2D piano roll matrix\n",
    "    :params format: format file type to write the stream\n",
    "    :params file_name: the file name of the output file\n",
    "    :params filepath: the output path of the directory holding the output files\n",
    "    :params step_duration: the size of the step on the x axis of the piano roll matrix\n",
    "    :returns None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the shape of the input matrix\n",
    "    rows, cols = matrix.shape\n",
    "\n",
    "    nulls = np.zeros((rows, 1))\n",
    "    matrix = np.hstack((matrix, nulls))\n",
    "    # Create two dictionaries. The first holds the notes that are on. The second holds each 'finished' note and its offset\n",
    "    active_notes = {}\n",
    "    note_list = {}\n",
    "\n",
    "    # Iterates through every member in the matrix\n",
    "    for col in range(cols - 1):\n",
    "        for row in range(rows - 1):\n",
    "            # Finds the midi pitch and creates a new note to represent the pitch and duration\n",
    "            midi_pitch = row\n",
    "            note = m21.note.Note(midi_pitch)\n",
    "            note.quarterLength = step_duration\n",
    "\n",
    "            # If this note is 'on':\n",
    "            if matrix[row, col] == 1:\n",
    "                \n",
    "                # Checks if the note has already been turned on, or is active.\n",
    "                if midi_pitch in active_notes:\n",
    "                    \n",
    "                    # If already active, updates the step duration of the note in the dictionary\n",
    "                    lst = active_notes[midi_pitch]\n",
    "                    lst[0] = lst[0] + step_duration\n",
    "                    active_notes[midi_pitch] = lst\n",
    "\n",
    "                # If newly activated, then adds the note duration and offset items to the midi pitch key in the dictionary\n",
    "                else:\n",
    "                    note.offset = col * step_duration\n",
    "                    active_notes[midi_pitch] = [note.quarterLength, note.offset]\n",
    "\n",
    "            # If the member is off but still in acitve notes, creates a new note and removes it from the dictionary\n",
    "            elif midi_pitch in active_notes:\n",
    "                # Grabs the duration and offset of the note and creates a new note object with duraiton, offset, midi pitch attributes\n",
    "                lst = active_notes[midi_pitch]\n",
    "                note = m21.note.Note( midi_pitch)\n",
    "                note.quarterLength = lst[0]\n",
    "                note.offset = lst[1]\n",
    "                # Adds this note to the note dictionary based off of the offset\n",
    "                note_list[note.offset] = note\n",
    "                del active_notes[midi_pitch]\n",
    "                    \n",
    "    # Creates a new stream and grabs the keys (offsets) and values (note onjects) from the note list dictionary\n",
    "    new_stream = m21.stream.Stream()\n",
    "    keys = list(note_list.keys())\n",
    "    notes = list(note_list.values())\n",
    "\n",
    "    # Iterates through every item in the dictionary\n",
    "    for i in range(len(note_list)):\n",
    "        # Inserts the note based off of its offset\n",
    "        new_stream.insert(keys[i], notes[i])\n",
    "\n",
    "    # Creates the filepath for the output file\n",
    "    path = os.path.join(filepath, file_name)\n",
    "\n",
    "    # Makes the directory if it doesn't exist\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "    # Writes the stream as a midi file to the path\n",
    "    new_stream.write(format, fp=path)\n",
    " \n",
    "\n",
    "convert_stream(piano_roll(SIZE, songs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_mapping(labels, path):\n",
    "    \"\"\"\n",
    "    Generate a mapping of the labels\n",
    "    \"\"\"\n",
    "    mappings = {}\n",
    "\n",
    "    # Finds the unique elements in the list\n",
    "    unique = list(set(labels))\n",
    "    \n",
    "    # Sets an integer value for every label\n",
    "    for i, symbol in enumerate(unique):\n",
    "        mappings[symbol] = i\n",
    "    # Opens the dicitonary in a new .json file\n",
    "    with open(path, \"w\") as fp:\n",
    "        json.dump(mappings, fp, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_int(labels):\n",
    "    \"\"\" \n",
    "    Converts the labels to an integer list\n",
    "    :returns label_ints: list of integers representing every label\n",
    "    \"\"\"\n",
    "    with open(LABEL_MAPPING_PATH, \"r\") as fp:\n",
    "        mappings = json.load(fp)\n",
    "\n",
    "    label_ints = []\n",
    "\n",
    "    for i, symbol in enumerate(mappings):\n",
    "        label_ints.append(mappings[symbol])\n",
    "\n",
    "    return label_ints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Input tensor dimensions:  torch.Size([3, 128, 512])\n",
      "Label tensor dimensions:  torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "def preprocess(data_path, songs, labels, no_songs_in_dir, size=SIZE):\n",
    "    \"\"\" \n",
    "    This function loads, encodes and saves the midi songs as the encoded version of the midi file.\n",
    "    :params data path containing the input midi files\n",
    "    :returns an output directory containing the encoded files.\n",
    "    \"\"\"\n",
    "    # Load the songs, labels and the counter\n",
    "    songs, labels, no_songs_in_dir = load_data(data_path)\n",
    "    index = 0\n",
    "    count = no_songs_in_dir[index]\n",
    "    \n",
    "    # Initialize the label mapping dictionaries\n",
    "    labels_mapping(labels, LABEL_MAPPING_PATH)\n",
    "    labels_as_ints = convert_labels_to_int(labels)\n",
    "    label = labels_as_ints[index]\n",
    "    \n",
    "    # Create empty label/matrix lists\n",
    "    label_list = []\n",
    "    matrix_list = []\n",
    "\n",
    "    for i, song in enumerate(songs):\n",
    "\n",
    "        # Filter out unaccetable durations for each song and create a piano roll matrix\n",
    "        song = acceptable_note_durations(song)\n",
    "        matrix = piano_roll(size, song)\n",
    "        \n",
    "        # Change the label if appropriate\n",
    "        if i == count:\n",
    "            if label != labels_as_ints[-1]:\n",
    "                index += 1\n",
    "                count = sum(no_songs_in_dir[:index + 1])\n",
    "                label = labels_as_ints[index]\n",
    "                \n",
    "        # Append to the lists   \n",
    "        label_list.append(label)\n",
    "        matrix_list.append(matrix)\n",
    "\n",
    "    # Convert to numpy arrays before creating tensors\n",
    "    input_array = np.array(matrix_list)\n",
    "    label_array = np.array(label_list)\n",
    "\n",
    "    inputs = torch.tensor(input_array)\n",
    "    labels = torch.tensor(label_array)\n",
    "    \n",
    "    print(\"Input tensor dimensions: \", inputs.shape)\n",
    "    print(\"Label tensor dimensions: \", labels.shape)\n",
    "    return inputs, labels\n",
    "\n",
    "inputs, labels = preprocess(PIANO_DATAPATH, songs, labels, no_songs_in_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Music_gen_WS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
