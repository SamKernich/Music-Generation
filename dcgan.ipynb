{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iffat\\AppData\\Local\\Temp\\ipykernel_26376\\404714875.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inputs = torch.load(\"input_tensors.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([263, 128, 512])\n",
      "Label tensor shape: torch.Size([263])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iffat\\AppData\\Local\\Temp\\ipykernel_26376\\404714875.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  labels = torch.load(\"label_tensors.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import music21 as m21\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the saved PyTorch tensors (input piano rolls)\n",
    "inputs = torch.load(\"input_tensors.pt\")\n",
    "labels = torch.load(\"label_tensors.pt\")\n",
    "\n",
    "# Check the shape of the tensors\n",
    "print(\"Input tensor shape:\", inputs.shape)\n",
    "print(\"Label tensor shape:\", labels.shape)\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "piano_roll_shape = (128, 512)  # MIDI pitch (128) by time steps (128)\n",
    "nz = 100  # Size of latent vector (noise)\n",
    "ngf = 64  # Generator feature map size\n",
    "ndf = 64  # Discriminator feature map size\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader created with batch size: 64\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create a DataLoader from the input tensors\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"DataLoader created with batch size:\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc = 1  # Number of channels in the input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: [batch_size, nz, 1, 1], nz is the size of the latent vector\n",
    "\n",
    "            # First layer: Transpose Conv to 1024 feature maps, 4x4 output\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, kernel_size=8, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),  # Output: [batch_size, ngf*8, 4, 4]\n",
    "\n",
    "            # Second layer: Transpose Conv to 512 feature maps, 8x8 output\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),  # Output: [batch_size, ngf*4, 8, 8]\n",
    "\n",
    "            # Third layer: Transpose Conv to 256 feature maps, 16x32 output\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=(4), stride=(2), padding=(1), bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),  # Output: [batch_size, ngf*2, 16, 32]\n",
    "\n",
    "            \n",
    "            # Fourth layer: Transpose Conv to 128 feature maps, 32x128 output\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, kernel_size=(4), stride=(2), padding=(1), bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),  # Output: [batch_size, ngf, 32, 128]\n",
    "\n",
    "            # Fifth layer: Transpose Conv to 1 channel, 128x512 output\n",
    "            nn.ConvTranspose2d(ngf, nc, kernel_size=(4), stride=(2), padding=(1), bias=False),\n",
    "            nn.Tanh()  # Output: [batch_size, 1, 128, 512]\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        print(f\"Input noise shape: {input.shape}\")  # Print the initial input shape (latent vector)\n",
    "        \n",
    "        x = self.main[0](input)  # First layer\n",
    "        print(f\"After first layer (ConvTranspose2d): {x.shape}\")\n",
    "        \n",
    "        x = self.main[1](x)  # First BatchNorm\n",
    "        x = self.main[2](x)  # First ReLU\n",
    "        print(f\"After first ReLU: {x.shape}\")\n",
    "        \n",
    "        x = self.main[3](x)  # Second layer\n",
    "        print(f\"After second layer (ConvTranspose2d): {x.shape}\")\n",
    "        \n",
    "        x = self.main[4](x)  # Second BatchNorm\n",
    "        x = self.main[5](x)  # Second ReLU\n",
    "        print(f\"After second ReLU: {x.shape}\")\n",
    "        \n",
    "        x = self.main[6](x)  # Third layer\n",
    "        print(f\"After third layer (ConvTranspose2d): {x.shape}\")\n",
    "        \n",
    "        x = self.main[7](x)  # Third BatchNorm\n",
    "        x = self.main[8](x)  # Third ReLU\n",
    "        print(f\"After third ReLU: {x.shape}\")\n",
    "        \n",
    "        x = self.main[9](x)  # Fourth layer\n",
    "        print(f\"After fourth layer (ConvTranspose2d): {x.shape}\")\n",
    "        \n",
    "        x = self.main[10](x)  # Fourth BatchNorm\n",
    "        x = self.main[11](x)  # Fourth ReLU\n",
    "        print(f\"After fourth ReLU: {x.shape}\")\n",
    "        \n",
    "        x = self.main[12](x)  # Fifth layer (final ConvTranspose2d)\n",
    "        print(f\"After fifth layer (ConvTranspose2d): {x.shape}\")\n",
    "        \n",
    "        x = self.main[13](x)  # Final Tanh activation\n",
    "        print(f\"After final Tanh: {x.shape}\")\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PianoRollDiscriminator(nn.Module):\n",
    "    def __init__(self, y_dim, pitch_range, time_steps=128):\n",
    "        super(PianoRollDiscriminator, self).__init__()\n",
    "        self.df_dim = 64\n",
    "        self.y_dim = y_dim\n",
    "        self.pitch_range = pitch_range\n",
    "        self.time_steps = time_steps\n",
    "\n",
    "        # Define the layers for the discriminator\n",
    "        self.conv1 = nn.Conv2d(1 + y_dim, self.df_dim, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(self.df_dim, self.df_dim * 2, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv3 = nn.Conv2d(self.df_dim * 2, self.df_dim * 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(self.df_dim * 4 * (time_steps // 8) * (pitch_range // 8), 1024)\n",
    "        self.fc2 = nn.Linear(1024 + y_dim, 1)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Concatenate piano roll x and label y\n",
    "        yb = y.view(y.size(0), self.y_dim, 1, 1)\n",
    "        yb = yb.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, yb], dim=1)  # Concatenate along the channel dimension\n",
    "\n",
    "        # Pass through convolutional layers\n",
    "        h1 = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        h2 = F.leaky_relu(self.conv2(h1), 0.2)\n",
    "        h3 = F.leaky_relu(self.conv3(h2), 0.2)\n",
    "\n",
    "        # Flatten and pass through fully connected layers\n",
    "        h3 = h3.view(h3.size(0), -1)  # Flatten\n",
    "        h4 = F.leaky_relu(self.fc1(h3))\n",
    "        h4 = torch.cat([h4, y], dim=1)  # Concatenate labels again\n",
    "\n",
    "        # Output layer\n",
    "        h5 = self.sigmoid(self.fc2(h4))\n",
    "\n",
    "        return h5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data shape: torch.Size([64, 1, 64, 64])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input noise shape: torch.Size([64, 100, 1, 1])\n",
      "After first layer (ConvTranspose2d): torch.Size([64, 512, 8, 8])\n",
      "After first ReLU: torch.Size([64, 512, 8, 8])\n",
      "After second layer (ConvTranspose2d): torch.Size([64, 256, 16, 16])\n",
      "After second ReLU: torch.Size([64, 256, 16, 16])\n",
      "After third layer (ConvTranspose2d): torch.Size([64, 128, 32, 32])\n",
      "After third ReLU: torch.Size([64, 128, 32, 32])\n",
      "After fourth layer (ConvTranspose2d): torch.Size([64, 64, 64, 64])\n",
      "After fourth ReLU: torch.Size([64, 64, 64, 64])\n",
      "After fifth layer (ConvTranspose2d): torch.Size([64, 1, 128, 128])\n",
      "After final Tanh: torch.Size([64, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "nz = 100  # Size of latent vector\n",
    "ngf = 64  # Generator feature map size\n",
    "nc = 1    # Number of channels (1 for grayscale, piano roll)\n",
    "\n",
    "# Create the Generator\n",
    "netG = Generator(nz=nz, ngf=ngf, nc=nc).to(device)\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Generate fake data and print layer-wise output shapes\n",
    "generated_data = netG(noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iffat\\OneDrive\\Desktop\\DCGAN\\Music-Generation\\venv\\Lib\\site-packages\\torch\\nn\\init.py:453: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "PianoRollGenerator.forward() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m64\u001b[39m, nz, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Generate fake data\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[43mnetG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Check the size of the generated data\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated data shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\iffat\\OneDrive\\Desktop\\DCGAN\\Music-Generation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iffat\\OneDrive\\Desktop\\DCGAN\\Music-Generation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: PianoRollGenerator.forward() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "nz = 100  # Size of latent vector\n",
    "ngf = 64  # Generator feature map size\n",
    "nc = 1    # Number of channels (1 for grayscale, piano roll)\n",
    "\n",
    "# Create the Generator\n",
    "netG = Generator(nz, ngf, nc).to(device)\n",
    "\n",
    "# Generate random noise\n",
    "noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Generate fake data\n",
    "generated_data = netG(noise)\n",
    "\n",
    "# Check the size of the generated data\n",
    "print(\"Generated data shape:\", generated_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([64])) that is different to the input size (torch.Size([9280])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), real_label, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     19\u001b[0m output \u001b[38;5;241m=\u001b[39m netD(real_data)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m lossD_real \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m lossD_real\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Train with fake data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\iffat\\OneDrive\\Desktop\\DCGAN\\Music-Generation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iffat\\OneDrive\\Desktop\\DCGAN\\Music-Generation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\iffat\\OneDrive\\Desktop\\DCGAN\\Music-Generation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:621\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\iffat\\OneDrive\\Desktop\\DCGAN\\Music-Generation\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3161\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3166\u001b[0m     )\n\u001b[0;32m   3168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3169\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([9280])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Labels for real and fake data\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update Discriminator\n",
    "        ############################\n",
    "        netD.zero_grad()\n",
    "\n",
    "        # Train with real data\n",
    "        real_data = data[0].unsqueeze(1).to(device).float()  # Add channel dimension for piano roll\n",
    "        batch_size = real_data.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device).float()\n",
    "        output = netD(real_data).view(-1)\n",
    "        lossD_real = criterion(output, label)\n",
    "        lossD_real.backward()\n",
    "\n",
    "        # Train with fake data\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device).float()\n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        lossD_fake = criterion(output, label)\n",
    "        lossD_fake.backward()\n",
    "\n",
    "        # Update Discriminator\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update Generator\n",
    "        ############################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # Generator wants the discriminator to classify its output as real\n",
    "        output = netD(fake_data).view(-1)\n",
    "        lossG = criterion(output, label)\n",
    "        lossG.backward()\n",
    "\n",
    "        # Update Generator\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Print training stats\n",
    "        if i % 100 == 0:\n",
    "            print(f\"[{epoch}/{epochs}][{i}/{len(dataloader)}] Loss_D: {lossD_real + lossD_fake:.4f} Loss_G: {lossG:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
