{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import music21\n",
    "import matplotlib as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "torch.cuda.empty_cache()\n",
    "cudnn.benchmark = True  # Optimise for hardware\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 400\n",
    "NOISE_DIM= 100\n",
    "NUM_CLASSES = 18\n",
    "BETA1 = 0.5 # Hyperparamter for adam optimizer\n",
    "LR = 0.004 # Might need to adjust\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(NOISE_DIM, 128 * 8 * 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(128 * 8 * 16)  # Batch Normalization\n",
    "        )\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # Upsample to 16x32\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)   # Upsample to 32x64\n",
    "        self.conv2_bn = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv3 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)     # Upsample to 64x128\n",
    "        self.conv4 = nn.ConvTranspose2d(1, 1, kernel_size=4, stride=2, padding=1)      # Upsample to 128x256\n",
    "\n",
    "        self.tanh_layer = nn.Tanh()\n",
    "\n",
    "    def forward(self, noise):\n",
    "        x = self.fc(noise)\n",
    "        x = x.view(-1, 128, 8, 16)  # Reshape for convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv1_bn(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_bn(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        out = self.conv4(x)\n",
    "        output = torch.sigmoid(out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1)  # (N, 64, 64, 128)\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1) # (N, 128, 32, 64)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.fc = nn.Linear(128 * 32 * 64, 1)  # Output layer\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.conv1(img)\n",
    "        \n",
    "        x = self.conv1_bn(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)  # (N, 64, 64, 128)\n",
    "       \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.conv2_bn(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)  # (N, 128, 32, 64)\n",
    "        \n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = torch.sigmoid(self.fc(x))\n",
    "        \n",
    "        \n",
    "        return x  # Sigmoid for binary classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "inputs_seq = torch.load(\"Input_tensors.pt\")\n",
    "labels_seq = torch.load(\"Labels_tensors.pt\")\n",
    "dataset = TensorDataset(inputs_seq, labels_seq)\n",
    "\n",
    "#Split into batches\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Check that the data is loaded correctly\n",
    "print(\"Number of input pianorolls: \", len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize models and optimizers\n",
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "gen = []\n",
    "criterion = nn.BCELoss()\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(10):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Prepare real data\n",
    "        real_cpu = data[0].to(device).float().unsqueeze(1)\n",
    "        b_size = real_cpu.size(0)\n",
    "        \n",
    "        #if b_size != BATCH_SIZE:\n",
    "           # print(f\"Skipping batch {i} due to insufficient size: {b_size}\")\n",
    "            #continue\n",
    "        \n",
    "        # Labels for real and fake\n",
    "        real_label = torch.full((b_size,), 0.9, dtype=torch.float, device=device)\n",
    "        fake_label = torch.full((b_size,), 0.15, dtype=torch.float, device=device)\n",
    "\n",
    "        # (1) Update D network\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Train with real data\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        errD_real = criterion(output, real_label)\n",
    "        errD_real.backward()\n",
    "\n",
    "        # Train with fake data\n",
    "        noise = torch.randn(b_size, NOISE_DIM, device=device)\n",
    "        fake = netG(noise)\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        errD_fake = criterion(output, fake_label)\n",
    "        errD_fake.backward()\n",
    "\n",
    "        # Update D\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # (2) Update G network\n",
    "        netG.zero_grad()\n",
    "        output = netD(fake).view(-1)\n",
    "        errG = criterion(output, real_label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 2 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch, 100, i, len(dataloader), errD.item(), errG.item()))\n",
    "            \n",
    "        \n",
    "        # Generate samples after a certain number of epochs\n",
    "        with torch.no_grad():\n",
    "            fixed_noise = torch.randn(BATCH_SIZE, NOISE_DIM, device=device)\n",
    "            generated_samples = netG(fixed_noise)\n",
    "            gen.append(generated_samples)\n",
    "            # Here you can visualize `generated_samples` using matplotlib or similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import music21 as m21\n",
    "import os\n",
    "OUTPUT_FILES = \"training_files\"\n",
    "TIME_STEP = 0.25\n",
    "def convert_stream(matrix, format=\"midi\", file_name='output.mid',filepath=OUTPUT_FILES, step_duration=TIME_STEP):\n",
    "    \"\"\"\n",
    "    Converts the piano roll matrix back into a music 21 stream. Writes this stream to a midi file.\n",
    "    :params matrix: 2D piano roll matrix\n",
    "    :params format: format file type to write the stream\n",
    "    :params file_name: the file name of the output file\n",
    "    :params filepath: the output path of the directory holding the output files\n",
    "    :params step_duration: the size of the step on the x axis of the piano roll matrix\n",
    "    :returns None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the shape of the input matrix\n",
    "    \n",
    "    rows, cols = matrix.shape\n",
    "    matrix = (matrix > 0.51).float()\n",
    "    nulls = np.zeros((rows, 1))\n",
    "    matrix = np.hstack((matrix, nulls))\n",
    "    # Create two dictionaries. The first holds the notes that are on. The second holds each 'finished' note and its offset\n",
    "    active_notes = {}\n",
    "    note_list = {}\n",
    "\n",
    "    # Iterates through every member in the matrix\n",
    "    for col in range(cols - 1):\n",
    "        for row in range(rows - 1):\n",
    "            # Finds the midi pitch and creates a new note to represent the pitch and duration\n",
    "            midi_pitch = row\n",
    "            note = m21.note.Note(midi_pitch)\n",
    "            note.quarterLength = step_duration\n",
    "\n",
    "            # If this note is 'on':\n",
    "            if matrix[row, col] == 1:\n",
    "                # remove midi pitches outside the range of a piano\n",
    "                if midi_pitch < 21 or midi_pitch > 95:\n",
    "                    continue\n",
    "                # Checks if the note has already been turned on, or is active.\n",
    "                if midi_pitch in active_notes:\n",
    "                    \n",
    "                    # If already active, updates the step duration of the note in the dictionary\n",
    "                    lst = active_notes[midi_pitch]\n",
    "                    lst[0] = lst[0] + step_duration\n",
    "                    active_notes[midi_pitch] = lst\n",
    "\n",
    "                # If newly activated, then adds the note duration and offset items to the midi pitch key in the dictionary\n",
    "                else:\n",
    "                    note.offset = col * step_duration\n",
    "                    active_notes[midi_pitch] = [note.quarterLength, note.offset]\n",
    "\n",
    "            # If the member is off but still in acitve notes, creates a new note and removes it from the dictionary\n",
    "            elif midi_pitch in active_notes:\n",
    "                # Grabs the duration and offset of the note and creates a new note object with duraiton, offset, midi pitch attributes\n",
    "                lst = active_notes[midi_pitch]\n",
    "                note = m21.note.Note( midi_pitch)\n",
    "                note.quarterLength = lst[0]\n",
    "                note.offset = lst[1]\n",
    "                # Adds this note to the note dictionary based off of the offset\n",
    "                note_list[note.offset] = note\n",
    "                del active_notes[midi_pitch]\n",
    "                    \n",
    "    # Creates a new stream and grabs the keys (offsets) and values (note onjects) from the note list dictionary\n",
    "    new_stream = m21.stream.Stream()\n",
    "    keys = list(note_list.keys())\n",
    "    notes = list(note_list.values())\n",
    "\n",
    "    # Iterates through every item in the dictionary\n",
    "    for i in range(len(note_list)):\n",
    "        # Inserts the note based off of its offset\n",
    "        new_stream.insert(keys[i], notes[i])\n",
    "\n",
    "    # Creates the filepath for the output file\n",
    "    path = os.path.join(filepath, file_name)\n",
    "\n",
    "    # Makes the directory if it doesn't exist\n",
    "    os.makedirs(filepath, exist_ok=True)\n",
    "\n",
    "    # Writes the stream as a midi file to the path\n",
    "    new_stream.write(format, fp=path)\n",
    "\n",
    "fixed_noise = torch.randn(BATCH_SIZE, NOISE_DIM, device=device)\n",
    "\n",
    "output = netG(fixed_noise)\n",
    "print(output)\n",
    "song = output[0][0]\n",
    "print(song.shape)\n",
    "convert_stream(song.cpu(), file_name=\"output8.mid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
